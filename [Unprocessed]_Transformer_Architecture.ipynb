{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "[Unprocessed] Transformer Architecture.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKX3lg8mB13n"
      },
      "source": [
        "### Connect to drive and setup training file:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7aeXVy-GB1Tv",
        "outputId": "5569ffb4-76a3-477c-de84-8ebef742fba0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51kSQ9h5B5id"
      },
      "source": [
        "### Importing the libraries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5nH_PLvO9qT",
        "outputId": "99021499-07c3-40d5-c84e-f01a8d4252af"
      },
      "source": [
        "installed = False\n",
        "\n",
        "if not installed:\n",
        "    !rm -rf indic_nlp_library indic_nlp_resources >> /dev/null\n",
        "    !git clone \"https://github.com/anoopkunchukuttan/indic_nlp_resources.git\" --quiet\n",
        "    !git clone \"https://github.com/anoopkunchukuttan/indic_nlp_library\" --quiet\n",
        "    !pip install -r \"./indic_nlp_library/requirements.txt\" >> /dev/null\n",
        "    !pip install indic-nlp-library >> /dev/null\n",
        "    !pip install Morfessor >> /dev/null"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aq-ieosa-1dD"
      },
      "source": [
        "from indicnlp.tokenize import indic_tokenize\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "from indicnlp import loader\n",
        "from indicnlp import common\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import indicnlp\n",
        "import random\n",
        "import torch\n",
        "import nltk\n",
        "import time\n",
        "import math\n",
        "import copy\n",
        "import sys\n",
        "import csv\n",
        "import gc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JYjFxvMJuZj",
        "outputId": "dbc4f838-3030-4ab5-b3c4-4904e42f2533"
      },
      "source": [
        "nltk.download('punkt', quiet=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIQwzHuI8J6T"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCoO8TRKHsOf"
      },
      "source": [
        "INDIC_NLP_LIB_HOME   =  \"./indic_nlp_library\"\n",
        "INDIC_NLP_RESOURCES  =  \"./indic_nlp_resources\"\n",
        "\n",
        "# Add indicnlp to system path:\n",
        "sys.path.append(INDIC_NLP_LIB_HOME)\n",
        "\n",
        "# Point the indicnlp resources:\n",
        "common.set_resources_path(INDIC_NLP_RESOURCES)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzifVhv7_rXP"
      },
      "source": [
        "hi_word2id = {}\n",
        "hi_id2word = {}\n",
        "en_word2id = {}\n",
        "en_id2word = {}\n",
        "\n",
        "hi_word2freq = {}\n",
        "en_word2freq = {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnG-V_LQAa7r"
      },
      "source": [
        "### Adding Start Word and Stop Word:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KD7Gt7oYs9mJ"
      },
      "source": [
        "# Adding start, stop words in hindi vocab:\n",
        "hi_word2id.update({'__<<init>>__': 0})\n",
        "hi_id2word.update({0: '__<<init>>__'})\n",
        "hi_word2id.update({'__<<stop>>__': 1})\n",
        "hi_id2word.update({1: '__<<stop>>__'})\n",
        "hi_word2id.update({'__<<unknown>>__': 2})\n",
        "hi_id2word.update({2: '__<<unknown>>__'})\n",
        "hi_word2id.update({'__<<padding>>__': 3})\n",
        "hi_id2word.update({3: '__<<padding>>__'})\n",
        "\n",
        "# Adding start, stop words in english vocab:\n",
        "en_word2id.update({'__<<init>>__': 0})\n",
        "en_id2word.update({0: '__<<init>>__'})\n",
        "en_word2id.update({'__<<stop>>__': 1})\n",
        "en_id2word.update({1: '__<<stop>>__'})\n",
        "en_word2id.update({'__<<unknown>>__': 2})\n",
        "en_id2word.update({2: '__<<unknown>>__'})\n",
        "en_word2id.update({'__<<padding>>__': 3})\n",
        "en_id2word.update({3: '__<<padding>>__'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eHsS_tXBeSq"
      },
      "source": [
        "### Loading the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpxJmZtIEtRi"
      },
      "source": [
        "# dataset = []\n",
        "# with open('./train.csv', 'r') as file:\n",
        "#     dataset = np.array([[r[1], r[2]] for r in csv.reader(file)])[1::]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKGYDSBHl9Qg",
        "outputId": "8a47496d-3c0a-4bac-efa7-aeef7f2d7b49"
      },
      "source": [
        "hi_dataset = []\n",
        "with open('./drive/MyDrive/dataset/2/train.hi', encoding='utf8') as file:\n",
        "    hi_dataset = [s.strip('\\n').replace('  ', ' ') for s in file.readlines()]\n",
        "\n",
        "hi_dataset[0:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['बड़ा व्यवसाय और अपराध',\n",
              " 'बड़ा व्यवसाय !',\n",
              " 'हम सब पर इसका प्रभाव पड़ता है ।',\n",
              " 'यह हमारी सहायता करता है — और हमें हानि भी पहुँचाता है ।',\n",
              " 'और कुछ ऐसी बातें हैं जो हम उसके बारे में कर सकते हैं ।',\n",
              " 'एक विशाल , या “ बड़े ” निगम के पास शायद १,५०,००,००,००० डॉलर की सम्पत्ति हो ।',\n",
              " 'बहुतों के पास इससे भी अधिक होता है ।',\n",
              " 'इस प्रकार का धन शक्\\u200dति का प्रतीक है ।',\n",
              " 'विशाल निगम अन्य देशों से लड़ाई करते हैं — जिसमें उनकी जीत होती है ।',\n",
              " 'इसमें कोई आश्\\u200dचर्य नहीं कि बहुतों को उन पर शक है !']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grAvQ8EdmceO",
        "outputId": "748d7a3c-e379-44a5-ce00-d640d3f6f309"
      },
      "source": [
        "en_dataset = []\n",
        "with open('./drive/MyDrive/dataset/2/train.en', encoding='utf8') as file:\n",
        "    en_dataset = [s.strip('\\n').replace('  ', ' ') for s in file.readlines()]\n",
        "\n",
        "en_dataset[0:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Big Business and Crime',\n",
              " 'BIG BUSINESS !',\n",
              " 'It affects all of us . It helps us \\u200b — and it harms us .',\n",
              " 'And there are things we can do about it . A giant , or “ big , ” corporation may have assets worth $ 1,500,000,000 .',\n",
              " 'Many have far more . That kind of money represents power .',\n",
              " 'Giant corporations have tussled with countries \\u200b — and won . No wonder so many are suspicious of them !',\n",
              " 'Yet in some ways big business has created the world we know .',\n",
              " 'It builds railroads , controls oil and in many lands provides electricity , gas and transportation .',\n",
              " 'Because of it , a person can wear shoes made in Brazil and clothes made in Hong Kong , drive a Japanese automobile , eat tropical food , and drink German wines .',\n",
              " 'Thanks to big business , foreign travel is no longer the exclusive privilege of the rich .']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhXwsfCmhhaL"
      },
      "source": [
        "hi_ds = []\n",
        "en_ds = []\n",
        "for i in range(len(hi_dataset)):\n",
        "    if len(indic_tokenize.trivial_tokenize(hi_dataset[i])) > 80:\n",
        "        pass\n",
        "    else:\n",
        "        en_ds += [en_dataset[i]]\n",
        "        hi_ds += [hi_dataset[i]]\n",
        "    \n",
        "hi_dataset = hi_ds\n",
        "en_dataset = en_ds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tm4cCthsia8O",
        "outputId": "bc6ea661-2508-4053-fc49-13f39a065cad"
      },
      "source": [
        "len(hi_dataset), len(en_dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(509400, 509400)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2yMjTEas9mL"
      },
      "source": [
        "hi_word_seq = []\n",
        "hi_counter = 4\n",
        "for row in hi_dataset:\n",
        "\n",
        "    # Hindi Sentences:\n",
        "    temp = indic_tokenize.trivial_tokenize(row)\n",
        "    hi_word_seq += [temp]\n",
        "    \n",
        "    for word in temp:\n",
        "        if word not in hi_word2id.keys():\n",
        "            hi_word2id.update({word: hi_counter})\n",
        "            hi_id2word.update({hi_counter: word})\n",
        "            hi_counter += 1\n",
        "\n",
        "en_word_seq = []\n",
        "en_counter = 4\n",
        "for row in en_dataset:\n",
        "\n",
        "    # English Sentences:\n",
        "    temp = nltk.word_tokenize(row)\n",
        "    en_word_seq += [temp]\n",
        "\n",
        "    for word in temp:\n",
        "        if word not in en_word2id.keys():\n",
        "            en_word2id.update({word: en_counter})\n",
        "            en_id2word.update({en_counter: word})\n",
        "            en_counter += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2hrlAkwLlj6"
      },
      "source": [
        "hi_max = max([len(l) for l in hi_word_seq])\n",
        "en_max = max([len(l) for l in en_word_seq])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOhUqEvXMvtY",
        "outputId": "5580fe0e-8717-40e6-ca05-5575f6049e8e"
      },
      "source": [
        "print('Hi-Vocabulary Size:', len(hi_id2word))\n",
        "print('En-Vocabulary Size:', len(en_id2word))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hi-Vocabulary Size: 75925\n",
            "En-Vocabulary Size: 76498\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoMwjYh9gPmw"
      },
      "source": [
        "def get_indices_seq(seq, vocab):\n",
        "    \n",
        "    temp = []\n",
        "    for word in seq:\n",
        "        if word in vocab.keys():\n",
        "            temp += [vocab[word]]\n",
        "        else:\n",
        "            temp += [vocab['__<<unknown>>__']]\n",
        "\n",
        "    seq = torch.tensor(\n",
        "        [vocab['__<<init>>__']] + temp + [vocab['__<<stop>>__']]\n",
        "    )\n",
        "\n",
        "    return seq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTf-yPHcjAt8"
      },
      "source": [
        "def get_word_seq(seq, vocab):\n",
        "\n",
        "    temp = []\n",
        "    for index in seq:\n",
        "        temp += [vocab[int(index)]]\n",
        "\n",
        "    return temp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwnoQeKlvrGZ"
      },
      "source": [
        "train_seq_pairs = []\n",
        "\n",
        "for sent_id in range(len(hi_word_seq)):\n",
        "    train_seq_pairs += [[\n",
        "        get_indices_seq(hi_word_seq[sent_id], hi_word2id).to(device),\n",
        "        get_indices_seq(en_word_seq[sent_id], en_word2id).to(device) \n",
        "    ]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USbsIpwENlZv"
      },
      "source": [
        "train_seq_pairs_sorted = sorted(train_seq_pairs, key=lambda x: len(x[0]) + len(x[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4Z-3ATydlzg"
      },
      "source": [
        "BATCH_SIZE = 32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EmIFRETAQcQ"
      },
      "source": [
        "input_batches = []\n",
        "output_batches = []\n",
        "for i in range(0, len(train_seq_pairs_sorted) - (len(train_seq_pairs_sorted) % BATCH_SIZE), BATCH_SIZE):\n",
        "\n",
        "    hi_indices_sequences = [pair[0] for pair in train_seq_pairs_sorted[i:i+BATCH_SIZE]]\n",
        "    en_indices_sequences = [pair[1] for pair in train_seq_pairs_sorted[i:i+BATCH_SIZE]]\n",
        "\n",
        "    input_batches += [\n",
        "        torch.nn.utils.rnn.pad_sequence(\n",
        "            hi_indices_sequences, \n",
        "            batch_first=False, \n",
        "            padding_value=3\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    output_batches += [\n",
        "        torch.nn.utils.rnn.pad_sequence(\n",
        "            en_indices_sequences, \n",
        "            batch_first=False, \n",
        "            padding_value=3\n",
        "        )\n",
        "    ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uprmqlzE-ABZ",
        "outputId": "c80869eb-9758-4ba7-d416-b07a3b8c4d42"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat May  8 06:08:36 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0    38W / 300W |   2055MiB / 16160MiB |     29%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cATRFtjbPyE0"
      },
      "source": [
        "### Transformer Model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FR5tFQacK7zL"
      },
      "source": [
        "class Norm(nn.Module):\n",
        "    def __init__(self, d_model, eps = 1e-6):\n",
        "        super().__init__()\n",
        "    \n",
        "        self.size = d_model\n",
        "        \n",
        "        # create two learnable parameters to calibrate normalisation\n",
        "        self.alpha = nn.Parameter(torch.ones(self.size))\n",
        "        self.bias = nn.Parameter(torch.zeros(self.size))\n",
        "        \n",
        "        self.eps = eps\n",
        "    \n",
        "    def forward(self, x):\n",
        "        norm = self.alpha * (x - x.mean(dim=-1, keepdim=True)) \\\n",
        "        / (x.std(dim=-1, keepdim=True) + self.eps) + self.bias\n",
        "        return norm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-HTwqKdLTmi"
      },
      "source": [
        "def attention(q, k, v, d_k, mask=None, dropout=None):\n",
        "    \n",
        "    scores = torch.matmul(q, k.transpose(-2, -1)) /  math.sqrt(d_k)\n",
        "    \n",
        "    if mask is not None:\n",
        "        mask = mask.unsqueeze(1).to(device)\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "    \n",
        "    scores = F.softmax(scores, dim=-1)\n",
        "    \n",
        "    if dropout is not None:\n",
        "        scores = dropout(scores)\n",
        "        \n",
        "    output = torch.matmul(scores, v)\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNWqhMLGLZIl"
      },
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, heads, d_model, dropout = 0.1):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        self.d_k = d_model // heads\n",
        "        self.h = heads\n",
        "        \n",
        "        self.q_linear = nn.Linear(d_model, d_model)\n",
        "        self.v_linear = nn.Linear(d_model, d_model)\n",
        "        self.k_linear = nn.Linear(d_model, d_model)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.out = nn.Linear(d_model, d_model)\n",
        "    \n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        \n",
        "        bs = q.size(0)\n",
        "        \n",
        "        # perform linear operation and split into N heads\n",
        "        k = self.k_linear(k).view(bs, -1, self.h, self.d_k)\n",
        "        q = self.q_linear(q).view(bs, -1, self.h, self.d_k)\n",
        "        v = self.v_linear(v).view(bs, -1, self.h, self.d_k)\n",
        "        \n",
        "        # transpose to get dimensions bs * N * sl * d_model\n",
        "        k = k.transpose(1,2)\n",
        "        q = q.transpose(1,2)\n",
        "        v = v.transpose(1,2)\n",
        "\n",
        "        # calculate attention using function we will define next\n",
        "        scores = attention(q, k, v, self.d_k, mask, self.dropout)\n",
        "        # concatenate heads and put through final linear layer\n",
        "        concat = scores.transpose(1,2).contiguous()\\\n",
        "        .view(bs, -1, self.d_model)\n",
        "        output = self.out(concat)\n",
        "    \n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2ieKmArLcc7"
      },
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff=2048, dropout = 0.1):\n",
        "        super().__init__() \n",
        "    \n",
        "        # We set d_ff as a default to 2048\n",
        "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.dropout(F.relu(self.linear_1(x)))\n",
        "        x = self.linear_2(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCmArZRPZce6"
      },
      "source": [
        "class Embedder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.embed = nn.Embedding(vocab_size, d_model)\n",
        "    def forward(self, x):\n",
        "        return self.embed(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTrADzPuZfug"
      },
      "source": [
        "class PositionalEncoder(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_len = 200, dropout = 0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        # create constant 'pe' matrix with values dependant on \n",
        "        # pos and i\n",
        "        pe = torch.zeros(max_seq_len, d_model)\n",
        "        for pos in range(max_seq_len):\n",
        "            for i in range(0, d_model, 2):\n",
        "                pe[pos, i] = \\\n",
        "                math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
        "                pe[pos, i + 1] = \\\n",
        "                math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        " \n",
        "    \n",
        "    def forward(self, x):\n",
        "        # make embeddings relatively larger\n",
        "        x = x * math.sqrt(self.d_model)\n",
        "        #add constant to embedding\n",
        "        seq_len = x.size(1)\n",
        "        pe = Variable(self.pe[:,:seq_len], requires_grad=False)\n",
        "        if x.is_cuda:\n",
        "            pe.cuda()\n",
        "        x = x + pe\n",
        "        return self.dropout(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BOzUfltL8YK"
      },
      "source": [
        "def get_clones(module, N):\n",
        "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yz7VUDvPZrZv"
      },
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.norm_1 = Norm(d_model)\n",
        "        self.norm_2 = Norm(d_model)\n",
        "        self.attn = MultiHeadAttention(heads, d_model, dropout=dropout)\n",
        "        self.ff = FeedForward(d_model, dropout=dropout)\n",
        "        self.dropout_1 = nn.Dropout(dropout)\n",
        "        self.dropout_2 = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x, mask):\n",
        "        x2 = self.norm_1(x)\n",
        "        x = x + self.dropout_1(self.attn(x2,x2,x2,mask))\n",
        "        x2 = self.norm_2(x)\n",
        "        x = x + self.dropout_2(self.ff(x2))\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfAAn-U5ZuXe"
      },
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.norm_1 = Norm(d_model)\n",
        "        self.norm_2 = Norm(d_model)\n",
        "        self.norm_3 = Norm(d_model)\n",
        "        \n",
        "        self.dropout_1 = nn.Dropout(dropout)\n",
        "        self.dropout_2 = nn.Dropout(dropout)\n",
        "        self.dropout_3 = nn.Dropout(dropout)\n",
        "        \n",
        "        self.attn_1 = MultiHeadAttention(heads, d_model, dropout=dropout)\n",
        "        self.attn_2 = MultiHeadAttention(heads, d_model, dropout=dropout)\n",
        "        self.ff = FeedForward(d_model, dropout=dropout)\n",
        "\n",
        "    def forward(self, x, e_outputs, src_mask, trg_mask):\n",
        "        x2 = self.norm_1(x)\n",
        "        x = x + self.dropout_1(self.attn_1(x2, x2, x2, trg_mask))\n",
        "        x2 = self.norm_2(x)\n",
        "        x = x + self.dropout_2(self.attn_2(x2, e_outputs, e_outputs, src_mask))\n",
        "        x2 = self.norm_3(x)\n",
        "        x = x + self.dropout_3(self.ff(x2))\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVgL-cubLrAV"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, N, heads, dropout):\n",
        "        super().__init__()\n",
        "        self.N = N\n",
        "        self.embed = Embedder(vocab_size, d_model)\n",
        "        self.pe = PositionalEncoder(d_model, dropout=dropout)\n",
        "        self.layers = get_clones(EncoderLayer(d_model, heads, dropout), N)\n",
        "        self.norm = Norm(d_model)\n",
        "    def forward(self, src, mask):\n",
        "        x = self.embed(src)\n",
        "        x = self.pe(x)\n",
        "        for i in range(self.N):\n",
        "            x = self.layers[i](x, mask)\n",
        "        return self.norm(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YlqikIiLuSV"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, N, heads, dropout):\n",
        "        super().__init__()\n",
        "        self.N = N\n",
        "        self.embed = Embedder(vocab_size, d_model)\n",
        "        self.pe = PositionalEncoder(d_model, dropout=dropout)\n",
        "        self.layers = get_clones(DecoderLayer(d_model, heads, dropout), N)\n",
        "        self.norm = Norm(d_model)\n",
        "    def forward(self, trg, e_outputs, src_mask, trg_mask):\n",
        "        x = self.embed(trg)\n",
        "        x = self.pe(x)\n",
        "        for i in range(self.N):\n",
        "            x = self.layers[i](x, e_outputs, src_mask, trg_mask)\n",
        "        return self.norm(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeXqPeHwLwxU"
      },
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab, trg_vocab, d_model, N, heads, dropout):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(src_vocab, d_model, N, heads, dropout)\n",
        "        self.decoder = Decoder(trg_vocab, d_model, N, heads, dropout)\n",
        "        self.out = nn.Linear(d_model, trg_vocab)\n",
        "    def forward(self, src, trg, src_mask, trg_mask):\n",
        "        e_outputs = self.encoder(src, src_mask)\n",
        "        #print(\"DECODER\")\n",
        "        d_output = self.decoder(trg, e_outputs, src_mask, trg_mask)\n",
        "        output = self.out(d_output)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76rYwzrMNWPR"
      },
      "source": [
        "opt = {\n",
        "    'SGDR': True,\n",
        "    'epochs': 20,\n",
        "    'd_model': 512,\n",
        "    'n_layers': 6,\n",
        "    'heads': 8,\n",
        "    'dropout': 0.1,\n",
        "    'batchsize': 32,\n",
        "    'lr': 1e-4,\n",
        "    'k': 5,\n",
        "    'max_len': 100\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ajbsa3ClLzNu"
      },
      "source": [
        "def get_model(opt, src_vocab, trg_vocab):\n",
        "    \n",
        "    assert opt[\"d_model\"] % opt[\"heads\"] == 0\n",
        "    assert opt[\"dropout\"] < 1\n",
        "\n",
        "    model = Transformer(src_vocab, trg_vocab, opt[\"d_model\"], opt[\"n_layers\"], opt[\"heads\"], opt[\"dropout\"])\n",
        "       \n",
        "    for p in model.parameters():\n",
        "        if p.dim() > 1:\n",
        "            nn.init.xavier_uniform_(p)\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCFHy0bgYxq9"
      },
      "source": [
        "model = get_model(opt, len(hi_word2id), len(en_word2id)).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmC3VH6Shoqm"
      },
      "source": [
        "def nopeak_mask(size):\n",
        "    np_mask = np.triu(np.ones((1, size, size)),\n",
        "    k=1).astype('uint8')\n",
        "    np_mask =  Variable(torch.from_numpy(np_mask) == 0)\n",
        "    return np_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5zdK42ohE36"
      },
      "source": [
        "def create_masks(src, trg):\n",
        "    \n",
        "    src_mask = (src != hi_word2id['__<<padding>>__']).unsqueeze(-2)\n",
        "\n",
        "    if trg is not None:\n",
        "        trg_mask = (trg != en_word2id['__<<padding>>__']).unsqueeze(-2)\n",
        "        size = trg.size(1) # get seq_len for matrix\n",
        "        np_mask = nopeak_mask(size).to(device)\n",
        "        trg_mask = trg_mask & np_mask\n",
        "    else:\n",
        "        trg_mask = None\n",
        "    return src_mask, trg_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjbQPHbGiWlF"
      },
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=opt['lr'], betas=(0.9, 0.98), eps=1e-9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9D8DcnNgTzJY",
        "outputId": "df13fc14-247e-4f45-f47a-3f178e1c393d"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat May  8 06:08:40 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0    38W / 300W |   2679MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dYFnTvR85qI"
      },
      "source": [
        "torch.save(model.state_dict(), 'drive/MyDrive/unprocessed_transformer_model')\n",
        "torch.save(optimizer.state_dict(), 'drive/MyDrive/unprocessed_transformer_optim')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4_a7Yhq9Mo2"
      },
      "source": [
        "model.load_state_dict(torch.load('drive/MyDrive/unprocessed_transformer_model_9'))\n",
        "optimizer.load_state_dict(torch.load('drive/MyDrive/unprocessed_transformer_optim_9'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SntL7zRY90u",
        "outputId": "deff8342-e3b6-48f6-97ba-efb1d01c101e"
      },
      "source": [
        "model.train()\n",
        "for epoch in range(10, 20, 1):\n",
        "\n",
        "    total_loss = 0\n",
        "\n",
        "    indices = list(range(len(input_batches)))\n",
        "    random.shuffle(indices)\n",
        "\n",
        "    tic = time.time()\n",
        "\n",
        "    counter = 0\n",
        "\n",
        "    minibatch_X = input_batches[0]\n",
        "    minibatch_Y = output_batches[0]\n",
        "\n",
        "    for i in indices:\n",
        "\n",
        "        minibatch_X = input_batches[i].T\n",
        "        minibatch_Y = output_batches[i].T\n",
        "\n",
        "        src = minibatch_X\n",
        "        trg = minibatch_Y\n",
        "\n",
        "        # print('SRC SHAPE:', src.shape)\n",
        "        # print('TRG SHAPE:', trg.shape)\n",
        "\n",
        "        trg_input = trg[:, :-1]\n",
        "        # trg_input = trg[:, :]\n",
        "\n",
        "        src_mask, trg_mask = create_masks(src, trg_input)\n",
        "        preds = model(src, trg_input, src_mask, trg_mask)\n",
        "        \n",
        "        # print(src.shape)\n",
        "        # print(trg.shape)\n",
        "        # print(preds.shape)\n",
        "\n",
        "        # ys = trg[:, :].contiguous().view(-1)\n",
        "        ys = trg[:, 1:].contiguous().view(-1)\n",
        "        optimizer.zero_grad()\n",
        "        loss = F.cross_entropy(preds.view(-1, preds.size(-1)), ys, ignore_index=en_word2id['__<<padding>>__'])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        counter += 1\n",
        "\n",
        "        if counter % 200 == 0:\n",
        "            print('{}/{} Batches Processed.'.format(counter, len(input_batches)))\n",
        "\n",
        "    toc = time.time()\n",
        "\n",
        "    torch.save(model.state_dict(), 'drive/MyDrive/unprocessed_transformer_model_{}'.format(epoch))\n",
        "    torch.save(optimizer.state_dict(), 'drive/MyDrive/unprocessed_transformer_optim_{}'.format(epoch))\n",
        "\n",
        "    print('Epoch {}, Total Loss: {}, Total Time For This Epoch: {}'.format(epoch, total_loss, toc - tic))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "200/15918 Batches Processed.\n",
            "400/15918 Batches Processed.\n",
            "600/15918 Batches Processed.\n",
            "800/15918 Batches Processed.\n",
            "1000/15918 Batches Processed.\n",
            "1200/15918 Batches Processed.\n",
            "1400/15918 Batches Processed.\n",
            "1600/15918 Batches Processed.\n",
            "1800/15918 Batches Processed.\n",
            "2000/15918 Batches Processed.\n",
            "2200/15918 Batches Processed.\n",
            "2400/15918 Batches Processed.\n",
            "2600/15918 Batches Processed.\n",
            "2800/15918 Batches Processed.\n",
            "3000/15918 Batches Processed.\n",
            "3200/15918 Batches Processed.\n",
            "3400/15918 Batches Processed.\n",
            "3600/15918 Batches Processed.\n",
            "3800/15918 Batches Processed.\n",
            "4000/15918 Batches Processed.\n",
            "4200/15918 Batches Processed.\n",
            "4400/15918 Batches Processed.\n",
            "4600/15918 Batches Processed.\n",
            "4800/15918 Batches Processed.\n",
            "5000/15918 Batches Processed.\n",
            "5200/15918 Batches Processed.\n",
            "5400/15918 Batches Processed.\n",
            "5600/15918 Batches Processed.\n",
            "5800/15918 Batches Processed.\n",
            "6000/15918 Batches Processed.\n",
            "6200/15918 Batches Processed.\n",
            "6400/15918 Batches Processed.\n",
            "6600/15918 Batches Processed.\n",
            "6800/15918 Batches Processed.\n",
            "7000/15918 Batches Processed.\n",
            "7200/15918 Batches Processed.\n",
            "7400/15918 Batches Processed.\n",
            "7600/15918 Batches Processed.\n",
            "7800/15918 Batches Processed.\n",
            "8000/15918 Batches Processed.\n",
            "8200/15918 Batches Processed.\n",
            "8400/15918 Batches Processed.\n",
            "8600/15918 Batches Processed.\n",
            "8800/15918 Batches Processed.\n",
            "9000/15918 Batches Processed.\n",
            "9200/15918 Batches Processed.\n",
            "9400/15918 Batches Processed.\n",
            "9600/15918 Batches Processed.\n",
            "9800/15918 Batches Processed.\n",
            "10000/15918 Batches Processed.\n",
            "10200/15918 Batches Processed.\n",
            "10400/15918 Batches Processed.\n",
            "10600/15918 Batches Processed.\n",
            "10800/15918 Batches Processed.\n",
            "11000/15918 Batches Processed.\n",
            "11200/15918 Batches Processed.\n",
            "11400/15918 Batches Processed.\n",
            "11600/15918 Batches Processed.\n",
            "11800/15918 Batches Processed.\n",
            "12000/15918 Batches Processed.\n",
            "12200/15918 Batches Processed.\n",
            "12400/15918 Batches Processed.\n",
            "12600/15918 Batches Processed.\n",
            "12800/15918 Batches Processed.\n",
            "13000/15918 Batches Processed.\n",
            "13200/15918 Batches Processed.\n",
            "13400/15918 Batches Processed.\n",
            "13600/15918 Batches Processed.\n",
            "13800/15918 Batches Processed.\n",
            "14000/15918 Batches Processed.\n",
            "14200/15918 Batches Processed.\n",
            "14400/15918 Batches Processed.\n",
            "14600/15918 Batches Processed.\n",
            "14800/15918 Batches Processed.\n",
            "15000/15918 Batches Processed.\n",
            "15200/15918 Batches Processed.\n",
            "15400/15918 Batches Processed.\n",
            "15600/15918 Batches Processed.\n",
            "15800/15918 Batches Processed.\n",
            "Epoch 10, Total Loss: 41119.56582411844, Total Time For This Epoch: 1470.6384046077728\n",
            "200/15918 Batches Processed.\n",
            "400/15918 Batches Processed.\n",
            "600/15918 Batches Processed.\n",
            "800/15918 Batches Processed.\n",
            "1000/15918 Batches Processed.\n",
            "1200/15918 Batches Processed.\n",
            "1400/15918 Batches Processed.\n",
            "1600/15918 Batches Processed.\n",
            "1800/15918 Batches Processed.\n",
            "2000/15918 Batches Processed.\n",
            "2200/15918 Batches Processed.\n",
            "2400/15918 Batches Processed.\n",
            "2600/15918 Batches Processed.\n",
            "2800/15918 Batches Processed.\n",
            "3000/15918 Batches Processed.\n",
            "3200/15918 Batches Processed.\n",
            "3400/15918 Batches Processed.\n",
            "3600/15918 Batches Processed.\n",
            "3800/15918 Batches Processed.\n",
            "4000/15918 Batches Processed.\n",
            "4200/15918 Batches Processed.\n",
            "4400/15918 Batches Processed.\n",
            "4600/15918 Batches Processed.\n",
            "4800/15918 Batches Processed.\n",
            "5000/15918 Batches Processed.\n",
            "5200/15918 Batches Processed.\n",
            "5400/15918 Batches Processed.\n",
            "5600/15918 Batches Processed.\n",
            "5800/15918 Batches Processed.\n",
            "6000/15918 Batches Processed.\n",
            "6200/15918 Batches Processed.\n",
            "6400/15918 Batches Processed.\n",
            "6600/15918 Batches Processed.\n",
            "6800/15918 Batches Processed.\n",
            "7000/15918 Batches Processed.\n",
            "7200/15918 Batches Processed.\n",
            "7400/15918 Batches Processed.\n",
            "7600/15918 Batches Processed.\n",
            "7800/15918 Batches Processed.\n",
            "8000/15918 Batches Processed.\n",
            "8200/15918 Batches Processed.\n",
            "8400/15918 Batches Processed.\n",
            "8600/15918 Batches Processed.\n",
            "8800/15918 Batches Processed.\n",
            "9000/15918 Batches Processed.\n",
            "9200/15918 Batches Processed.\n",
            "9400/15918 Batches Processed.\n",
            "9600/15918 Batches Processed.\n",
            "9800/15918 Batches Processed.\n",
            "10000/15918 Batches Processed.\n",
            "10200/15918 Batches Processed.\n",
            "10400/15918 Batches Processed.\n",
            "10600/15918 Batches Processed.\n",
            "10800/15918 Batches Processed.\n",
            "11000/15918 Batches Processed.\n",
            "11200/15918 Batches Processed.\n",
            "11400/15918 Batches Processed.\n",
            "11600/15918 Batches Processed.\n",
            "11800/15918 Batches Processed.\n",
            "12000/15918 Batches Processed.\n",
            "12200/15918 Batches Processed.\n",
            "12400/15918 Batches Processed.\n",
            "12600/15918 Batches Processed.\n",
            "12800/15918 Batches Processed.\n",
            "13000/15918 Batches Processed.\n",
            "13200/15918 Batches Processed.\n",
            "13400/15918 Batches Processed.\n",
            "13600/15918 Batches Processed.\n",
            "13800/15918 Batches Processed.\n",
            "14000/15918 Batches Processed.\n",
            "14200/15918 Batches Processed.\n",
            "14400/15918 Batches Processed.\n",
            "14600/15918 Batches Processed.\n",
            "14800/15918 Batches Processed.\n",
            "15000/15918 Batches Processed.\n",
            "15200/15918 Batches Processed.\n",
            "15400/15918 Batches Processed.\n",
            "15600/15918 Batches Processed.\n",
            "15800/15918 Batches Processed.\n",
            "Epoch 11, Total Loss: 40835.542814654764, Total Time For This Epoch: 1468.5516481399536\n",
            "200/15918 Batches Processed.\n",
            "400/15918 Batches Processed.\n",
            "600/15918 Batches Processed.\n",
            "800/15918 Batches Processed.\n",
            "1000/15918 Batches Processed.\n",
            "1200/15918 Batches Processed.\n",
            "1400/15918 Batches Processed.\n",
            "1600/15918 Batches Processed.\n",
            "1800/15918 Batches Processed.\n",
            "2000/15918 Batches Processed.\n",
            "2200/15918 Batches Processed.\n",
            "2400/15918 Batches Processed.\n",
            "2600/15918 Batches Processed.\n",
            "2800/15918 Batches Processed.\n",
            "3000/15918 Batches Processed.\n",
            "3200/15918 Batches Processed.\n",
            "3400/15918 Batches Processed.\n",
            "3600/15918 Batches Processed.\n",
            "3800/15918 Batches Processed.\n",
            "4000/15918 Batches Processed.\n",
            "4200/15918 Batches Processed.\n",
            "4400/15918 Batches Processed.\n",
            "4600/15918 Batches Processed.\n",
            "4800/15918 Batches Processed.\n",
            "5000/15918 Batches Processed.\n",
            "5200/15918 Batches Processed.\n",
            "5400/15918 Batches Processed.\n",
            "5600/15918 Batches Processed.\n",
            "5800/15918 Batches Processed.\n",
            "6000/15918 Batches Processed.\n",
            "6200/15918 Batches Processed.\n",
            "6400/15918 Batches Processed.\n",
            "6600/15918 Batches Processed.\n",
            "6800/15918 Batches Processed.\n",
            "7000/15918 Batches Processed.\n",
            "7200/15918 Batches Processed.\n",
            "7400/15918 Batches Processed.\n",
            "7600/15918 Batches Processed.\n",
            "7800/15918 Batches Processed.\n",
            "8000/15918 Batches Processed.\n",
            "8200/15918 Batches Processed.\n",
            "8400/15918 Batches Processed.\n",
            "8600/15918 Batches Processed.\n",
            "8800/15918 Batches Processed.\n",
            "9000/15918 Batches Processed.\n",
            "9200/15918 Batches Processed.\n",
            "9400/15918 Batches Processed.\n",
            "9600/15918 Batches Processed.\n",
            "9800/15918 Batches Processed.\n",
            "10000/15918 Batches Processed.\n",
            "10200/15918 Batches Processed.\n",
            "10400/15918 Batches Processed.\n",
            "10600/15918 Batches Processed.\n",
            "10800/15918 Batches Processed.\n",
            "11000/15918 Batches Processed.\n",
            "11200/15918 Batches Processed.\n",
            "11400/15918 Batches Processed.\n",
            "11600/15918 Batches Processed.\n",
            "11800/15918 Batches Processed.\n",
            "12000/15918 Batches Processed.\n",
            "12200/15918 Batches Processed.\n",
            "12400/15918 Batches Processed.\n",
            "12600/15918 Batches Processed.\n",
            "12800/15918 Batches Processed.\n",
            "13000/15918 Batches Processed.\n",
            "13200/15918 Batches Processed.\n",
            "13400/15918 Batches Processed.\n",
            "13600/15918 Batches Processed.\n",
            "13800/15918 Batches Processed.\n",
            "14000/15918 Batches Processed.\n",
            "14200/15918 Batches Processed.\n",
            "14400/15918 Batches Processed.\n",
            "14600/15918 Batches Processed.\n",
            "14800/15918 Batches Processed.\n",
            "15000/15918 Batches Processed.\n",
            "15200/15918 Batches Processed.\n",
            "15400/15918 Batches Processed.\n",
            "15600/15918 Batches Processed.\n",
            "15800/15918 Batches Processed.\n",
            "Epoch 12, Total Loss: 40639.93238747795, Total Time For This Epoch: 1465.9943633079529\n",
            "200/15918 Batches Processed.\n",
            "400/15918 Batches Processed.\n",
            "600/15918 Batches Processed.\n",
            "800/15918 Batches Processed.\n",
            "1000/15918 Batches Processed.\n",
            "1200/15918 Batches Processed.\n",
            "1400/15918 Batches Processed.\n",
            "1600/15918 Batches Processed.\n",
            "1800/15918 Batches Processed.\n",
            "2000/15918 Batches Processed.\n",
            "2200/15918 Batches Processed.\n",
            "2400/15918 Batches Processed.\n",
            "2600/15918 Batches Processed.\n",
            "2800/15918 Batches Processed.\n",
            "3000/15918 Batches Processed.\n",
            "3200/15918 Batches Processed.\n",
            "3400/15918 Batches Processed.\n",
            "3600/15918 Batches Processed.\n",
            "3800/15918 Batches Processed.\n",
            "4000/15918 Batches Processed.\n",
            "4200/15918 Batches Processed.\n",
            "4400/15918 Batches Processed.\n",
            "4600/15918 Batches Processed.\n",
            "4800/15918 Batches Processed.\n",
            "5000/15918 Batches Processed.\n",
            "5200/15918 Batches Processed.\n",
            "5400/15918 Batches Processed.\n",
            "5600/15918 Batches Processed.\n",
            "5800/15918 Batches Processed.\n",
            "6000/15918 Batches Processed.\n",
            "6200/15918 Batches Processed.\n",
            "6400/15918 Batches Processed.\n",
            "6600/15918 Batches Processed.\n",
            "6800/15918 Batches Processed.\n",
            "7000/15918 Batches Processed.\n",
            "7200/15918 Batches Processed.\n",
            "7400/15918 Batches Processed.\n",
            "7600/15918 Batches Processed.\n",
            "7800/15918 Batches Processed.\n",
            "8000/15918 Batches Processed.\n",
            "8200/15918 Batches Processed.\n",
            "8400/15918 Batches Processed.\n",
            "8600/15918 Batches Processed.\n",
            "8800/15918 Batches Processed.\n",
            "9000/15918 Batches Processed.\n",
            "9200/15918 Batches Processed.\n",
            "9400/15918 Batches Processed.\n",
            "9600/15918 Batches Processed.\n",
            "9800/15918 Batches Processed.\n",
            "10000/15918 Batches Processed.\n",
            "10200/15918 Batches Processed.\n",
            "10400/15918 Batches Processed.\n",
            "10600/15918 Batches Processed.\n",
            "10800/15918 Batches Processed.\n",
            "11000/15918 Batches Processed.\n",
            "11200/15918 Batches Processed.\n",
            "11400/15918 Batches Processed.\n",
            "11600/15918 Batches Processed.\n",
            "11800/15918 Batches Processed.\n",
            "12000/15918 Batches Processed.\n",
            "12200/15918 Batches Processed.\n",
            "12400/15918 Batches Processed.\n",
            "12600/15918 Batches Processed.\n",
            "12800/15918 Batches Processed.\n",
            "13000/15918 Batches Processed.\n",
            "13200/15918 Batches Processed.\n",
            "13400/15918 Batches Processed.\n",
            "13600/15918 Batches Processed.\n",
            "13800/15918 Batches Processed.\n",
            "14000/15918 Batches Processed.\n",
            "14200/15918 Batches Processed.\n",
            "14400/15918 Batches Processed.\n",
            "14600/15918 Batches Processed.\n",
            "14800/15918 Batches Processed.\n",
            "15000/15918 Batches Processed.\n",
            "15200/15918 Batches Processed.\n",
            "15400/15918 Batches Processed.\n",
            "15600/15918 Batches Processed.\n",
            "15800/15918 Batches Processed.\n",
            "Epoch 13, Total Loss: 40416.53110406455, Total Time For This Epoch: 1468.635799407959\n",
            "200/15918 Batches Processed.\n",
            "400/15918 Batches Processed.\n",
            "600/15918 Batches Processed.\n",
            "800/15918 Batches Processed.\n",
            "1000/15918 Batches Processed.\n",
            "1200/15918 Batches Processed.\n",
            "1400/15918 Batches Processed.\n",
            "1600/15918 Batches Processed.\n",
            "1800/15918 Batches Processed.\n",
            "2000/15918 Batches Processed.\n",
            "2200/15918 Batches Processed.\n",
            "2400/15918 Batches Processed.\n",
            "2600/15918 Batches Processed.\n",
            "2800/15918 Batches Processed.\n",
            "3000/15918 Batches Processed.\n",
            "3200/15918 Batches Processed.\n",
            "3400/15918 Batches Processed.\n",
            "3600/15918 Batches Processed.\n",
            "3800/15918 Batches Processed.\n",
            "4000/15918 Batches Processed.\n",
            "4200/15918 Batches Processed.\n",
            "4400/15918 Batches Processed.\n",
            "4600/15918 Batches Processed.\n",
            "4800/15918 Batches Processed.\n",
            "5000/15918 Batches Processed.\n",
            "5200/15918 Batches Processed.\n",
            "5400/15918 Batches Processed.\n",
            "5600/15918 Batches Processed.\n",
            "5800/15918 Batches Processed.\n",
            "6000/15918 Batches Processed.\n",
            "6200/15918 Batches Processed.\n",
            "6400/15918 Batches Processed.\n",
            "6600/15918 Batches Processed.\n",
            "6800/15918 Batches Processed.\n",
            "7000/15918 Batches Processed.\n",
            "7200/15918 Batches Processed.\n",
            "7400/15918 Batches Processed.\n",
            "7600/15918 Batches Processed.\n",
            "7800/15918 Batches Processed.\n",
            "8000/15918 Batches Processed.\n",
            "8200/15918 Batches Processed.\n",
            "8400/15918 Batches Processed.\n",
            "8600/15918 Batches Processed.\n",
            "8800/15918 Batches Processed.\n",
            "9000/15918 Batches Processed.\n",
            "9200/15918 Batches Processed.\n",
            "9400/15918 Batches Processed.\n",
            "9600/15918 Batches Processed.\n",
            "9800/15918 Batches Processed.\n",
            "10000/15918 Batches Processed.\n",
            "10200/15918 Batches Processed.\n",
            "10400/15918 Batches Processed.\n",
            "10600/15918 Batches Processed.\n",
            "10800/15918 Batches Processed.\n",
            "11000/15918 Batches Processed.\n",
            "11200/15918 Batches Processed.\n",
            "11400/15918 Batches Processed.\n",
            "11600/15918 Batches Processed.\n",
            "11800/15918 Batches Processed.\n",
            "12000/15918 Batches Processed.\n",
            "12200/15918 Batches Processed.\n",
            "12400/15918 Batches Processed.\n",
            "12600/15918 Batches Processed.\n",
            "12800/15918 Batches Processed.\n",
            "13000/15918 Batches Processed.\n",
            "13200/15918 Batches Processed.\n",
            "13400/15918 Batches Processed.\n",
            "13600/15918 Batches Processed.\n",
            "13800/15918 Batches Processed.\n",
            "14000/15918 Batches Processed.\n",
            "14200/15918 Batches Processed.\n",
            "14400/15918 Batches Processed.\n",
            "14600/15918 Batches Processed.\n",
            "14800/15918 Batches Processed.\n",
            "15000/15918 Batches Processed.\n",
            "15200/15918 Batches Processed.\n",
            "15400/15918 Batches Processed.\n",
            "15600/15918 Batches Processed.\n",
            "15800/15918 Batches Processed.\n",
            "Epoch 14, Total Loss: 40113.81654199585, Total Time For This Epoch: 1469.0879352092743\n",
            "200/15918 Batches Processed.\n",
            "400/15918 Batches Processed.\n",
            "600/15918 Batches Processed.\n",
            "800/15918 Batches Processed.\n",
            "1000/15918 Batches Processed.\n",
            "1200/15918 Batches Processed.\n",
            "1400/15918 Batches Processed.\n",
            "1600/15918 Batches Processed.\n",
            "1800/15918 Batches Processed.\n",
            "2000/15918 Batches Processed.\n",
            "2200/15918 Batches Processed.\n",
            "2400/15918 Batches Processed.\n",
            "2600/15918 Batches Processed.\n",
            "2800/15918 Batches Processed.\n",
            "3000/15918 Batches Processed.\n",
            "3200/15918 Batches Processed.\n",
            "3400/15918 Batches Processed.\n",
            "3600/15918 Batches Processed.\n",
            "3800/15918 Batches Processed.\n",
            "4000/15918 Batches Processed.\n",
            "4200/15918 Batches Processed.\n",
            "4400/15918 Batches Processed.\n",
            "4600/15918 Batches Processed.\n",
            "4800/15918 Batches Processed.\n",
            "5000/15918 Batches Processed.\n",
            "5200/15918 Batches Processed.\n",
            "5400/15918 Batches Processed.\n",
            "5600/15918 Batches Processed.\n",
            "5800/15918 Batches Processed.\n",
            "6000/15918 Batches Processed.\n",
            "6200/15918 Batches Processed.\n",
            "6400/15918 Batches Processed.\n",
            "6600/15918 Batches Processed.\n",
            "6800/15918 Batches Processed.\n",
            "7000/15918 Batches Processed.\n",
            "7200/15918 Batches Processed.\n",
            "7400/15918 Batches Processed.\n",
            "7600/15918 Batches Processed.\n",
            "7800/15918 Batches Processed.\n",
            "8000/15918 Batches Processed.\n",
            "8200/15918 Batches Processed.\n",
            "8400/15918 Batches Processed.\n",
            "8600/15918 Batches Processed.\n",
            "8800/15918 Batches Processed.\n",
            "9000/15918 Batches Processed.\n",
            "9200/15918 Batches Processed.\n",
            "9400/15918 Batches Processed.\n",
            "9600/15918 Batches Processed.\n",
            "9800/15918 Batches Processed.\n",
            "10000/15918 Batches Processed.\n",
            "10200/15918 Batches Processed.\n",
            "10400/15918 Batches Processed.\n",
            "10600/15918 Batches Processed.\n",
            "10800/15918 Batches Processed.\n",
            "11000/15918 Batches Processed.\n",
            "11200/15918 Batches Processed.\n",
            "11400/15918 Batches Processed.\n",
            "11600/15918 Batches Processed.\n",
            "11800/15918 Batches Processed.\n",
            "12000/15918 Batches Processed.\n",
            "12200/15918 Batches Processed.\n",
            "12400/15918 Batches Processed.\n",
            "12600/15918 Batches Processed.\n",
            "12800/15918 Batches Processed.\n",
            "13000/15918 Batches Processed.\n",
            "13200/15918 Batches Processed.\n",
            "13400/15918 Batches Processed.\n",
            "13600/15918 Batches Processed.\n",
            "13800/15918 Batches Processed.\n",
            "14000/15918 Batches Processed.\n",
            "14200/15918 Batches Processed.\n",
            "14400/15918 Batches Processed.\n",
            "14600/15918 Batches Processed.\n",
            "14800/15918 Batches Processed.\n",
            "15000/15918 Batches Processed.\n",
            "15200/15918 Batches Processed.\n",
            "15400/15918 Batches Processed.\n",
            "15600/15918 Batches Processed.\n",
            "15800/15918 Batches Processed.\n",
            "Epoch 15, Total Loss: 39887.69503734447, Total Time For This Epoch: 1464.231109380722\n",
            "200/15918 Batches Processed.\n",
            "400/15918 Batches Processed.\n",
            "600/15918 Batches Processed.\n",
            "800/15918 Batches Processed.\n",
            "1000/15918 Batches Processed.\n",
            "1200/15918 Batches Processed.\n",
            "1400/15918 Batches Processed.\n",
            "1600/15918 Batches Processed.\n",
            "1800/15918 Batches Processed.\n",
            "2000/15918 Batches Processed.\n",
            "2200/15918 Batches Processed.\n",
            "2400/15918 Batches Processed.\n",
            "2600/15918 Batches Processed.\n",
            "2800/15918 Batches Processed.\n",
            "3000/15918 Batches Processed.\n",
            "3200/15918 Batches Processed.\n",
            "3400/15918 Batches Processed.\n",
            "3600/15918 Batches Processed.\n",
            "3800/15918 Batches Processed.\n",
            "4000/15918 Batches Processed.\n",
            "4200/15918 Batches Processed.\n",
            "4400/15918 Batches Processed.\n",
            "4600/15918 Batches Processed.\n",
            "4800/15918 Batches Processed.\n",
            "5000/15918 Batches Processed.\n",
            "5200/15918 Batches Processed.\n",
            "5400/15918 Batches Processed.\n",
            "5600/15918 Batches Processed.\n",
            "5800/15918 Batches Processed.\n",
            "6000/15918 Batches Processed.\n",
            "6200/15918 Batches Processed.\n",
            "6400/15918 Batches Processed.\n",
            "6600/15918 Batches Processed.\n",
            "6800/15918 Batches Processed.\n",
            "7000/15918 Batches Processed.\n",
            "7200/15918 Batches Processed.\n",
            "7400/15918 Batches Processed.\n",
            "7600/15918 Batches Processed.\n",
            "7800/15918 Batches Processed.\n",
            "8000/15918 Batches Processed.\n",
            "8200/15918 Batches Processed.\n",
            "8400/15918 Batches Processed.\n",
            "8600/15918 Batches Processed.\n",
            "8800/15918 Batches Processed.\n",
            "9000/15918 Batches Processed.\n",
            "9200/15918 Batches Processed.\n",
            "9400/15918 Batches Processed.\n",
            "9600/15918 Batches Processed.\n",
            "9800/15918 Batches Processed.\n",
            "10000/15918 Batches Processed.\n",
            "10200/15918 Batches Processed.\n",
            "10400/15918 Batches Processed.\n",
            "10600/15918 Batches Processed.\n",
            "10800/15918 Batches Processed.\n",
            "11000/15918 Batches Processed.\n",
            "11200/15918 Batches Processed.\n",
            "11400/15918 Batches Processed.\n",
            "11600/15918 Batches Processed.\n",
            "11800/15918 Batches Processed.\n",
            "12000/15918 Batches Processed.\n",
            "12200/15918 Batches Processed.\n",
            "12400/15918 Batches Processed.\n",
            "12600/15918 Batches Processed.\n",
            "12800/15918 Batches Processed.\n",
            "13000/15918 Batches Processed.\n",
            "13200/15918 Batches Processed.\n",
            "13400/15918 Batches Processed.\n",
            "13600/15918 Batches Processed.\n",
            "13800/15918 Batches Processed.\n",
            "14000/15918 Batches Processed.\n",
            "14200/15918 Batches Processed.\n",
            "14400/15918 Batches Processed.\n",
            "14600/15918 Batches Processed.\n",
            "14800/15918 Batches Processed.\n",
            "15000/15918 Batches Processed.\n",
            "15200/15918 Batches Processed.\n",
            "15400/15918 Batches Processed.\n",
            "15600/15918 Batches Processed.\n",
            "15800/15918 Batches Processed.\n",
            "Epoch 16, Total Loss: 39744.17322778981, Total Time For This Epoch: 1468.0723836421967\n",
            "200/15918 Batches Processed.\n",
            "400/15918 Batches Processed.\n",
            "600/15918 Batches Processed.\n",
            "800/15918 Batches Processed.\n",
            "1000/15918 Batches Processed.\n",
            "1200/15918 Batches Processed.\n",
            "1400/15918 Batches Processed.\n",
            "1600/15918 Batches Processed.\n",
            "1800/15918 Batches Processed.\n",
            "2000/15918 Batches Processed.\n",
            "2200/15918 Batches Processed.\n",
            "2400/15918 Batches Processed.\n",
            "2600/15918 Batches Processed.\n",
            "2800/15918 Batches Processed.\n",
            "3000/15918 Batches Processed.\n",
            "3200/15918 Batches Processed.\n",
            "3400/15918 Batches Processed.\n",
            "3600/15918 Batches Processed.\n",
            "3800/15918 Batches Processed.\n",
            "4000/15918 Batches Processed.\n",
            "4200/15918 Batches Processed.\n",
            "4400/15918 Batches Processed.\n",
            "4600/15918 Batches Processed.\n",
            "4800/15918 Batches Processed.\n",
            "5000/15918 Batches Processed.\n",
            "5200/15918 Batches Processed.\n",
            "5400/15918 Batches Processed.\n",
            "5600/15918 Batches Processed.\n",
            "5800/15918 Batches Processed.\n",
            "6000/15918 Batches Processed.\n",
            "6200/15918 Batches Processed.\n",
            "6400/15918 Batches Processed.\n",
            "6600/15918 Batches Processed.\n",
            "6800/15918 Batches Processed.\n",
            "7000/15918 Batches Processed.\n",
            "7200/15918 Batches Processed.\n",
            "7400/15918 Batches Processed.\n",
            "7600/15918 Batches Processed.\n",
            "7800/15918 Batches Processed.\n",
            "8000/15918 Batches Processed.\n",
            "8200/15918 Batches Processed.\n",
            "8400/15918 Batches Processed.\n",
            "8600/15918 Batches Processed.\n",
            "8800/15918 Batches Processed.\n",
            "9000/15918 Batches Processed.\n",
            "9200/15918 Batches Processed.\n",
            "9400/15918 Batches Processed.\n",
            "9600/15918 Batches Processed.\n",
            "9800/15918 Batches Processed.\n",
            "10000/15918 Batches Processed.\n",
            "10200/15918 Batches Processed.\n",
            "10400/15918 Batches Processed.\n",
            "10600/15918 Batches Processed.\n",
            "10800/15918 Batches Processed.\n",
            "11000/15918 Batches Processed.\n",
            "11200/15918 Batches Processed.\n",
            "11400/15918 Batches Processed.\n",
            "11600/15918 Batches Processed.\n",
            "11800/15918 Batches Processed.\n",
            "12000/15918 Batches Processed.\n",
            "12200/15918 Batches Processed.\n",
            "12400/15918 Batches Processed.\n",
            "12600/15918 Batches Processed.\n",
            "12800/15918 Batches Processed.\n",
            "13000/15918 Batches Processed.\n",
            "13200/15918 Batches Processed.\n",
            "13400/15918 Batches Processed.\n",
            "13600/15918 Batches Processed.\n",
            "13800/15918 Batches Processed.\n",
            "14000/15918 Batches Processed.\n",
            "14200/15918 Batches Processed.\n",
            "14400/15918 Batches Processed.\n",
            "14600/15918 Batches Processed.\n",
            "14800/15918 Batches Processed.\n",
            "15000/15918 Batches Processed.\n",
            "15200/15918 Batches Processed.\n",
            "15400/15918 Batches Processed.\n",
            "15600/15918 Batches Processed.\n",
            "15800/15918 Batches Processed.\n",
            "Epoch 17, Total Loss: 39526.170925421175, Total Time For This Epoch: 1491.7600765228271\n",
            "200/15918 Batches Processed.\n",
            "400/15918 Batches Processed.\n",
            "600/15918 Batches Processed.\n",
            "800/15918 Batches Processed.\n",
            "1000/15918 Batches Processed.\n",
            "1200/15918 Batches Processed.\n",
            "1400/15918 Batches Processed.\n",
            "1600/15918 Batches Processed.\n",
            "1800/15918 Batches Processed.\n",
            "2000/15918 Batches Processed.\n",
            "2200/15918 Batches Processed.\n",
            "2400/15918 Batches Processed.\n",
            "2600/15918 Batches Processed.\n",
            "2800/15918 Batches Processed.\n",
            "3000/15918 Batches Processed.\n",
            "3200/15918 Batches Processed.\n",
            "3400/15918 Batches Processed.\n",
            "3600/15918 Batches Processed.\n",
            "3800/15918 Batches Processed.\n",
            "4000/15918 Batches Processed.\n",
            "4200/15918 Batches Processed.\n",
            "4400/15918 Batches Processed.\n",
            "4600/15918 Batches Processed.\n",
            "4800/15918 Batches Processed.\n",
            "5000/15918 Batches Processed.\n",
            "5200/15918 Batches Processed.\n",
            "5400/15918 Batches Processed.\n",
            "5600/15918 Batches Processed.\n",
            "5800/15918 Batches Processed.\n",
            "6000/15918 Batches Processed.\n",
            "6200/15918 Batches Processed.\n",
            "6400/15918 Batches Processed.\n",
            "6600/15918 Batches Processed.\n",
            "6800/15918 Batches Processed.\n",
            "7000/15918 Batches Processed.\n",
            "7200/15918 Batches Processed.\n",
            "7400/15918 Batches Processed.\n",
            "7600/15918 Batches Processed.\n",
            "7800/15918 Batches Processed.\n",
            "8000/15918 Batches Processed.\n",
            "8200/15918 Batches Processed.\n",
            "8400/15918 Batches Processed.\n",
            "8600/15918 Batches Processed.\n",
            "8800/15918 Batches Processed.\n",
            "9000/15918 Batches Processed.\n",
            "9200/15918 Batches Processed.\n",
            "9400/15918 Batches Processed.\n",
            "9600/15918 Batches Processed.\n",
            "9800/15918 Batches Processed.\n",
            "10000/15918 Batches Processed.\n",
            "10200/15918 Batches Processed.\n",
            "10400/15918 Batches Processed.\n",
            "10600/15918 Batches Processed.\n",
            "10800/15918 Batches Processed.\n",
            "11000/15918 Batches Processed.\n",
            "11200/15918 Batches Processed.\n",
            "11400/15918 Batches Processed.\n",
            "11600/15918 Batches Processed.\n",
            "11800/15918 Batches Processed.\n",
            "12000/15918 Batches Processed.\n",
            "12200/15918 Batches Processed.\n",
            "12400/15918 Batches Processed.\n",
            "12600/15918 Batches Processed.\n",
            "12800/15918 Batches Processed.\n",
            "13000/15918 Batches Processed.\n",
            "13200/15918 Batches Processed.\n",
            "13400/15918 Batches Processed.\n",
            "13600/15918 Batches Processed.\n",
            "13800/15918 Batches Processed.\n",
            "14000/15918 Batches Processed.\n",
            "14200/15918 Batches Processed.\n",
            "14400/15918 Batches Processed.\n",
            "14600/15918 Batches Processed.\n",
            "14800/15918 Batches Processed.\n",
            "15000/15918 Batches Processed.\n",
            "15200/15918 Batches Processed.\n",
            "15400/15918 Batches Processed.\n",
            "15600/15918 Batches Processed.\n",
            "15800/15918 Batches Processed.\n",
            "Epoch 18, Total Loss: 39357.35421847459, Total Time For This Epoch: 1497.9400045871735\n",
            "200/15918 Batches Processed.\n",
            "400/15918 Batches Processed.\n",
            "600/15918 Batches Processed.\n",
            "800/15918 Batches Processed.\n",
            "1000/15918 Batches Processed.\n",
            "1200/15918 Batches Processed.\n",
            "1400/15918 Batches Processed.\n",
            "1600/15918 Batches Processed.\n",
            "1800/15918 Batches Processed.\n",
            "2000/15918 Batches Processed.\n",
            "2200/15918 Batches Processed.\n",
            "2400/15918 Batches Processed.\n",
            "2600/15918 Batches Processed.\n",
            "2800/15918 Batches Processed.\n",
            "3000/15918 Batches Processed.\n",
            "3200/15918 Batches Processed.\n",
            "3400/15918 Batches Processed.\n",
            "3600/15918 Batches Processed.\n",
            "3800/15918 Batches Processed.\n",
            "4000/15918 Batches Processed.\n",
            "4200/15918 Batches Processed.\n",
            "4400/15918 Batches Processed.\n",
            "4600/15918 Batches Processed.\n",
            "4800/15918 Batches Processed.\n",
            "5000/15918 Batches Processed.\n",
            "5200/15918 Batches Processed.\n",
            "5400/15918 Batches Processed.\n",
            "5600/15918 Batches Processed.\n",
            "5800/15918 Batches Processed.\n",
            "6000/15918 Batches Processed.\n",
            "6200/15918 Batches Processed.\n",
            "6400/15918 Batches Processed.\n",
            "6600/15918 Batches Processed.\n",
            "6800/15918 Batches Processed.\n",
            "7000/15918 Batches Processed.\n",
            "7200/15918 Batches Processed.\n",
            "7400/15918 Batches Processed.\n",
            "7600/15918 Batches Processed.\n",
            "7800/15918 Batches Processed.\n",
            "8000/15918 Batches Processed.\n",
            "8200/15918 Batches Processed.\n",
            "8400/15918 Batches Processed.\n",
            "8600/15918 Batches Processed.\n",
            "8800/15918 Batches Processed.\n",
            "9000/15918 Batches Processed.\n",
            "9200/15918 Batches Processed.\n",
            "9400/15918 Batches Processed.\n",
            "9600/15918 Batches Processed.\n",
            "9800/15918 Batches Processed.\n",
            "10000/15918 Batches Processed.\n",
            "10200/15918 Batches Processed.\n",
            "10400/15918 Batches Processed.\n",
            "10600/15918 Batches Processed.\n",
            "10800/15918 Batches Processed.\n",
            "11000/15918 Batches Processed.\n",
            "11200/15918 Batches Processed.\n",
            "11400/15918 Batches Processed.\n",
            "11600/15918 Batches Processed.\n",
            "11800/15918 Batches Processed.\n",
            "12000/15918 Batches Processed.\n",
            "12200/15918 Batches Processed.\n",
            "12400/15918 Batches Processed.\n",
            "12600/15918 Batches Processed.\n",
            "12800/15918 Batches Processed.\n",
            "13000/15918 Batches Processed.\n",
            "13200/15918 Batches Processed.\n",
            "13400/15918 Batches Processed.\n",
            "13600/15918 Batches Processed.\n",
            "13800/15918 Batches Processed.\n",
            "14000/15918 Batches Processed.\n",
            "14200/15918 Batches Processed.\n",
            "14400/15918 Batches Processed.\n",
            "14600/15918 Batches Processed.\n",
            "14800/15918 Batches Processed.\n",
            "15000/15918 Batches Processed.\n",
            "15200/15918 Batches Processed.\n",
            "15400/15918 Batches Processed.\n",
            "15600/15918 Batches Processed.\n",
            "15800/15918 Batches Processed.\n",
            "Epoch 19, Total Loss: 39128.28004118288, Total Time For This Epoch: 1501.1907811164856\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgedivMBHDnk"
      },
      "source": [
        "def init_vars(src, model):\n",
        "    \n",
        "    # src: indices sequence (may have padding)\n",
        "    # shape: (1, 1, src_len)\n",
        "\n",
        "    init_tok = en_word2id['__<<init>>__']\n",
        "    # start token\n",
        "    # shape: scalar\n",
        "\n",
        "    src_mask = (src != hi_word2id['__<<padding>>__']).unsqueeze(-2)\n",
        "    # mask padding sequences:\n",
        "    # shape: (1, 1, src_len)\n",
        "    # \n",
        "    # =======================================\n",
        "    # Description:\n",
        "    # =======================================\n",
        "    # \n",
        "    # if src: [[[w1, w2, <p>, <p>, <p>]]] then src_mask: [[[True, True]]]\n",
        "\n",
        "    e_output = model.encoder(src, src_mask)\n",
        "    # encoder outputs:\n",
        "    # 512 dimensional vector for each word:\n",
        "    # shape: (1, src_len, 512)\n",
        "\n",
        "    outputs = torch.LongTensor([[init_tok]]).to(device)\n",
        "    # initial output for target sentence:\n",
        "    # eg. [__<<init>>__]\n",
        "    # shape: (1, 1)\n",
        "\n",
        "    trg_mask = nopeak_mask(1)\n",
        "    # initial output mask:\n",
        "    # shape: (1, 1, 1)\n",
        "\n",
        "    out = model.out(model.decoder(outputs, e_output, src_mask, trg_mask))\n",
        "    # model output:\n",
        "    # shape: (1, 1, target_vocab)\n",
        "\n",
        "    out = F.softmax(out, dim=-1)\n",
        "    # model normalized output:\n",
        "    # shape: (1, 1, target_vocab)\n",
        "\n",
        "    probs, ix = out[:, -1].data.topk(opt['k'])\n",
        "    # probs: k topmost probabilities\n",
        "    # shape: (1, K)\n",
        "\n",
        "    # ix: k topmost indices\n",
        "    # shape: (1, K)\n",
        "\n",
        "    log_scores = torch.Tensor([math.log(prob) for prob in probs.data[0]]).unsqueeze(0)\n",
        "    # log scores: score of the sequence so far.\n",
        "    # shape: (1, K)\n",
        "\n",
        "    outputs = torch.zeros(opt[\"k\"], opt[\"max_len\"]).long().to(device)\n",
        "    # buffer for top k outputs: \n",
        "    # shape: (K, max_len)\n",
        "\n",
        "    outputs[:, 0] = init_tok\n",
        "    outputs[:, 1] = ix[0]\n",
        "    # set first word = '__<<init>>__' and predict next word:\n",
        "    # shape: (K, max_len)\n",
        "    \n",
        "    e_outputs = torch.zeros(opt[\"k\"], e_output.size(-2), e_output.size(-1)).to(device)\n",
        "    # allocate tensor for encoder outputs\n",
        "    # shape: (K, src_len, 512)\n",
        "\n",
        "    e_outputs[:, :] = e_output[0]\n",
        "    # broadcast encoder outputs for each sequences\n",
        "    # shape(K, src_len, 512)\n",
        "\n",
        "    return outputs, e_outputs, log_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEHpqPUTHJ4o"
      },
      "source": [
        "def k_best_outputs(outputs, out, log_scores, i, k):\n",
        "    \n",
        "    # outputs: (K, max_len)\n",
        "    # out: (K, i, target_vocab)\n",
        "    # log_scores: (1, K)\n",
        "    # i: current length so far.\n",
        "    # k: beam width.\n",
        "\n",
        "    probs, ix = out[:, -1].data.topk(k)\n",
        "    # probs: top-k predictions for each of top k previous probabilities\n",
        "    # shape: (k, k)\n",
        "\n",
        "    # ix: top-k indices for each of top k previous probabilities\n",
        "    # shape: (k, k)\n",
        "\n",
        "    log_probs = torch.Tensor([math.log(p) for p in probs.data.view(-1)]).view(k, -1) + log_scores.transpose(0, 1)\n",
        "    # calculating log_probabilities:\n",
        "    # shape: (k, k)\n",
        "    \n",
        "    k_probs, k_ix = log_probs.view(-1).topk(k)\n",
        "    # k_probs shape: (5,)\n",
        "    # k_ix shape: (5,)\n",
        "\n",
        "    row = k_ix // k\n",
        "    col = k_ix % k\n",
        "\n",
        "    outputs[:, :i] = outputs[row, :i]\n",
        "    outputs[:, i] = ix[row, col]\n",
        "\n",
        "    log_scores = k_probs.unsqueeze(0)\n",
        "\n",
        "    # outputs: (K, max_len)\n",
        "    # log_scores: (1, K)\n",
        "    \n",
        "    return outputs, log_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbkndXROHNXn"
      },
      "source": [
        "def beam_search(src, model, opt):\n",
        "\n",
        "    outputs, e_outputs, log_scores = init_vars(src, model)\n",
        "    # initialize the generation of sentence and allocate buffers\n",
        "    # outputs: (K, max_len)\n",
        "    # e_outputs: (K, seq_len, 512)\n",
        "    # log_scores: (1, K)\n",
        "\n",
        "    eos_tok = en_word2id['__<<stop>>__']\n",
        "    # target '__<<stop>>__' token:\n",
        "    # shape: scalar\n",
        "\n",
        "    src_mask = (src != hi_word2id['__<<padding>>__']).unsqueeze(-2)\n",
        "    # mask padding sequences:\n",
        "    # shape: (1, 1, src_len)\n",
        "    # \n",
        "    # =======================================\n",
        "    # Description:\n",
        "    # =======================================\n",
        "    # \n",
        "    # if src: [[[w1, w2, <p>, <p>, <p>]]] then src_mask: [[[True, True]]]\n",
        "\n",
        "    ind = None\n",
        "    for i in range(2, opt[\"max_len\"]):\n",
        "    \n",
        "        trg_mask = nopeak_mask(i)\n",
        "        # Masking the target upto current length:\n",
        "        # shape: (1, i, i) where i represents output length so far.\n",
        "\n",
        "        out = model.out(model.decoder(outputs[:,:i], e_outputs, src_mask, trg_mask))\n",
        "        # out: (K, i, dict) where i represents output length so far.\n",
        "\n",
        "        out = F.softmax(out, dim=-1)\n",
        "        # softmax: (K, i, dict) where i represents output length so far.\n",
        "\n",
        "        outputs, log_scores = k_best_outputs(outputs, out, log_scores, i, opt[\"k\"])\n",
        "        # finding next K best words:\n",
        "        # outputs: (K, max_len)\n",
        "        # log_scores: (1, K)\n",
        "\n",
        "        ones = (outputs.long()==eos_tok).nonzero()\n",
        "\n",
        "        sentence_lengths = torch.zeros(len(outputs), dtype=torch.long).cuda()\n",
        "        for vec in ones:\n",
        "            i = vec[0]\n",
        "            if int(sentence_lengths[i]) == 0:\n",
        "                sentence_lengths[i] = vec[1]\n",
        "\n",
        "        num_finished_sentences = len([s for s in sentence_lengths if s > 0])\n",
        "\n",
        "        if num_finished_sentences == opt[\"k\"]:\n",
        "            alpha = 0.7\n",
        "            div = 1/(sentence_lengths.type_as(log_scores)**alpha)\n",
        "            _, ind = torch.max(log_scores * div, 1)\n",
        "            ind = ind.data[0]\n",
        "            break\n",
        "\n",
        "    # print(num_finished_sentences)\n",
        "\n",
        "    if ind is None:\n",
        "        length = (outputs.long()[0]==eos_tok).nonzero()[0]\n",
        "        return ' '.join([en_id2word[int(tok)] for tok in outputs[0][1:length]])\n",
        "    \n",
        "    else:\n",
        "        length = (outputs.long()[ind]==eos_tok).nonzero()[0]\n",
        "        return ' '.join([en_id2word[int(tok)] for tok in outputs[ind][1:length]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQqNaz7Mgg_b"
      },
      "source": [
        "def translate_sentence(sentence, model, opt):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    indexed = []\n",
        "    sentence = indic_tokenize.trivial_tokenize(sentence)\n",
        "    \n",
        "    for tok in sentence:\n",
        "        if tok in hi_word2id.keys():\n",
        "            if hi_word2id[tok] != 0:\n",
        "                indexed.append(hi_word2id[tok])\n",
        "        else:\n",
        "            indexed.append(hi_word2id['__<<unknown>>__'])\n",
        "\n",
        "    sentence = torch.Tensor([[0] + indexed + [1]]).long().to(device)\n",
        "\n",
        "    # print('SENT', sentence)\n",
        "\n",
        "    sentence = beam_search(sentence, model, opt)\n",
        "\n",
        "    return sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CE_a9zWg_Vn"
      },
      "source": [
        "ntests = 100\n",
        "\n",
        "test_sents = []\n",
        "with open('file1.txt', 'r') as file:\n",
        "    test_sents = [line.strip('\\n').split('\\t')[1] for line in file.readlines()[2000:2000 + ntests]]\n",
        "\n",
        "for line in test_sents:\n",
        "    print('===================================')\n",
        "    print('In dataset:', line in hi_dataset)\n",
        "    print('     Hindi:', line)\n",
        "    print('   English:', translate_sentence(line, model, opt))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "zloUZ4H1JQ9n",
        "outputId": "0e9cf1b1-971d-40e3-a5a7-2d814be0d291"
      },
      "source": [
        "sent = \"क्या हमें अपने ग्रह के भविष्य के बारे में चिंता करने की ज़रूरत है ?\"\n",
        "\n",
        "print(sent in hi_dataset)\n",
        "\n",
        "translate_sentence(\"क्या हमें अपने ग्रह के भविष्य के बारे में चिंता करने की ज़रूरत है ?\", model, opt)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Do we need to be concerned about the future of our planet ?'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1u5VfEU1Lvg0"
      },
      "source": [
        "for i in range(20):\n",
        "    print('===========================================')\n",
        "    print(translate_sentence(dataset[i][0], model, opt))\n",
        "    print(dataset[i][1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZH6aT2c_p7ut"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}