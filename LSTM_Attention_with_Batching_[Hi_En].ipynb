{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM Attention with Batching [Hi-En].ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKX3lg8mB13n"
      },
      "source": [
        "### Connect to drive and setup training file:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7aeXVy-GB1Tv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be50d379-f39d-433f-b8eb-9896d75640d9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGruk1xKDeBW"
      },
      "source": [
        "# Getting the training dataset file from gdrive\n",
        "!unzip -o ./drive/MyDrive/train.zip >> /dev/null\n",
        "!unzip -o ./drive/MyDrive/hindistatements.zip >> /dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51kSQ9h5B5id"
      },
      "source": [
        "### Importing the libraries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5nH_PLvO9qT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce3d7a44-f5a0-4513-ac64-d6bcfa870f98"
      },
      "source": [
        "installed = False\n",
        "\n",
        "if not installed:\n",
        "    !rm -rf indic_nlp_library indic_nlp_resources >> /dev/null\n",
        "    !git clone \"https://github.com/anoopkunchukuttan/indic_nlp_resources.git\" --quiet\n",
        "    !git clone \"https://github.com/anoopkunchukuttan/indic_nlp_library\" --quiet\n",
        "    !pip install -r \"./indic_nlp_library/requirements.txt\" >> /dev/null\n",
        "    !pip install indic-nlp-library >> /dev/null\n",
        "    !pip install Morfessor >> /dev/null"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aq-ieosa-1dD"
      },
      "source": [
        "from indicnlp.tokenize import indic_tokenize\n",
        "import torch.nn.functional as F\n",
        "from indicnlp import loader\n",
        "from indicnlp import common\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import indicnlp\n",
        "import random\n",
        "import torch\n",
        "import nltk\n",
        "import time\n",
        "import sys\n",
        "import csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JYjFxvMJuZj",
        "outputId": "06771d17-572b-4b55-cb66-ea979892616a"
      },
      "source": [
        "nltk.download('punkt', quiet=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIQwzHuI8J6T"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCoO8TRKHsOf"
      },
      "source": [
        "INDIC_NLP_LIB_HOME   =  \"./indic_nlp_library\"\n",
        "INDIC_NLP_RESOURCES  =  \"./indic_nlp_resources\"\n",
        "\n",
        "# Add indicnlp to system path:\n",
        "sys.path.append(INDIC_NLP_LIB_HOME)\n",
        "\n",
        "# Point the indicnlp resources:\n",
        "common.set_resources_path(INDIC_NLP_RESOURCES)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzifVhv7_rXP"
      },
      "source": [
        "hi_word2id = {}\n",
        "hi_id2word = {}\n",
        "en_word2id = {}\n",
        "en_id2word = {}\n",
        "\n",
        "hi_word2freq = {}\n",
        "en_word2freq = {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnG-V_LQAa7r"
      },
      "source": [
        "### Adding Start Word and Stop Word:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KD7Gt7oYs9mJ"
      },
      "source": [
        "# Adding start, stop words in hindi vocab:\n",
        "hi_word2id.update({'__<<init>>__': 0})\n",
        "hi_id2word.update({0: '__<<init>>__'})\n",
        "hi_word2id.update({'__<<stop>>__': 1})\n",
        "hi_id2word.update({1: '__<<stop>>__'})\n",
        "hi_word2id.update({'__<<unknown>>__': 2})\n",
        "hi_id2word.update({2: '__<<unknown>>__'})\n",
        "hi_word2id.update({'__<<padding>>__': 3})\n",
        "hi_id2word.update({3: '__<<padding>>__'})\n",
        "\n",
        "# Adding start, stop words in english vocab:\n",
        "en_word2id.update({'__<<init>>__': 0})\n",
        "en_id2word.update({0: '__<<init>>__'})\n",
        "en_word2id.update({'__<<stop>>__': 1})\n",
        "en_id2word.update({1: '__<<stop>>__'})\n",
        "en_word2id.update({'__<<unknown>>__': 2})\n",
        "en_id2word.update({2: '__<<unknown>>__'})\n",
        "en_word2id.update({'__<<padding>>__': 3})\n",
        "en_id2word.update({3: '__<<padding>>__'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eHsS_tXBeSq"
      },
      "source": [
        "### Loading the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpxJmZtIEtRi"
      },
      "source": [
        "dataset = []\n",
        "with open('./train.csv', 'r') as file:\n",
        "    dataset = np.array([[r[1], r[2]] for r in csv.reader(file)])[1::]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2yMjTEas9mL"
      },
      "source": [
        "hi_word_seq = []\n",
        "en_word_seq = []\n",
        "\n",
        "hi_counter = 4\n",
        "en_counter = 4\n",
        "for row in dataset:\n",
        "    \n",
        "    # Hindi Sentences:\n",
        "    temp = indic_tokenize.trivial_tokenize(row[0])\n",
        "    hi_word_seq += [temp]\n",
        "    \n",
        "    for word in temp:\n",
        "        if word not in hi_word2id.keys():\n",
        "            hi_word2id.update({word: hi_counter})\n",
        "            hi_id2word.update({hi_counter: word})\n",
        "            hi_counter += 1\n",
        "\n",
        "    # English Sentences:\n",
        "    temp = nltk.word_tokenize(row[1])\n",
        "    en_word_seq += [temp]\n",
        "\n",
        "    for word in temp:\n",
        "        if word not in en_word2id.keys():\n",
        "            en_word2id.update({word: en_counter})\n",
        "            en_id2word.update({en_counter: word})\n",
        "            en_counter += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2hrlAkwLlj6"
      },
      "source": [
        "hi_max = max([len(l) for l in hi_word_seq])\n",
        "en_max = max([len(l) for l in en_word_seq])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOhUqEvXMvtY",
        "outputId": "20f960db-9d35-4a10-a77f-09a2d5ab969a"
      },
      "source": [
        "print('Hi-Vocabulary Size:', len(hi_id2word))\n",
        "print('En-Vocabulary Size:', len(en_id2word))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hi-Vocabulary Size: 46384\n",
            "En-Vocabulary Size: 40802\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoMwjYh9gPmw"
      },
      "source": [
        "def get_indices_seq(seq, vocab):\n",
        "    \n",
        "    temp = []\n",
        "    for word in seq:\n",
        "        if word in vocab.keys():\n",
        "            temp += [vocab[word]]\n",
        "        else:\n",
        "            temp += [vocab['__<<unknown>>__']]\n",
        "\n",
        "    seq = torch.tensor(\n",
        "        [vocab['__<<init>>__']] + temp + [vocab['__<<stop>>__']]\n",
        "    )\n",
        "\n",
        "    return seq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTf-yPHcjAt8"
      },
      "source": [
        "def get_word_seq(seq, vocab):\n",
        "\n",
        "    temp = []\n",
        "    for index in seq:\n",
        "        temp += [vocab[int(index)]]\n",
        "\n",
        "    return temp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwnoQeKlvrGZ"
      },
      "source": [
        "train_seq_pairs = []\n",
        "\n",
        "for sent_id in range(len(hi_word_seq)):\n",
        "    train_seq_pairs += [[\n",
        "        get_indices_seq(hi_word_seq[sent_id], hi_word2id).to(device),\n",
        "        get_indices_seq(en_word_seq[sent_id], en_word2id).to(device) \n",
        "    ]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USbsIpwENlZv"
      },
      "source": [
        "train_seq_pairs_sorted = sorted(train_seq_pairs, key=lambda x: len(x[0]) + len(x[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4Z-3ATydlzg"
      },
      "source": [
        "BATCH_SIZE = 64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EmIFRETAQcQ"
      },
      "source": [
        "input_batches = []\n",
        "output_batches = []\n",
        "for i in range(0, len(train_seq_pairs_sorted) - (len(train_seq_pairs_sorted) % BATCH_SIZE), BATCH_SIZE):\n",
        "\n",
        "    hi_indices_sequences = [pair[0] for pair in train_seq_pairs_sorted[i:i+BATCH_SIZE]]\n",
        "    en_indices_sequences = [pair[1] for pair in train_seq_pairs_sorted[i:i+BATCH_SIZE]]\n",
        "\n",
        "    input_batches += [\n",
        "        torch.nn.utils.rnn.pad_sequence(\n",
        "            hi_indices_sequences, \n",
        "            batch_first=False, \n",
        "            padding_value=3\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    output_batches += [\n",
        "        torch.nn.utils.rnn.pad_sequence(\n",
        "            en_indices_sequences, \n",
        "            batch_first=False, \n",
        "            padding_value=3\n",
        "        )\n",
        "    ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uprmqlzE-ABZ",
        "outputId": "83511d76-8afb-46a0-8966-9561e00cb5da"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat May  8 07:55:50 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0    40W / 300W |   1455MiB / 16160MiB |     13%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cATRFtjbPyE0"
      },
      "source": [
        "### Encoder Model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VcxkfiXs9mQ"
      },
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, embedding_size, hidden_size, p):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        self.rnn = nn.LSTM(embedding_size, hidden_size, 1, bidirectional=True)\n",
        "\n",
        "        self.fc_hidden = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        self.fc_cell = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        self.dropout = nn.Dropout(p)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (seq_length, N) where N is batch size\n",
        "\n",
        "        # Convert integers to embedding vectors:\n",
        "        embedding = self.dropout(self.embedding(x))\n",
        "        # embedding shape: (seq_length, N, embedding_size)\n",
        "\n",
        "        # \n",
        "        encoder_states, (hidden, cell) = self.rnn(embedding)\n",
        "        # outputs shape: (seq_length, N, hidden_size)\n",
        "\n",
        "        # Use forward, backward cells and hidden through a linear layer\n",
        "        # so that it can be input to the decoder which is not bidirectional\n",
        "        # Also using index slicing ([idx:idx+1]) to keep the dimension\n",
        "        hidden = self.fc_hidden(torch.cat((hidden[0:1], hidden[1:2]), dim=2))\n",
        "        cell = self.fc_cell(torch.cat((cell[0:1], cell[1:2]), dim=2))\n",
        "\n",
        "        return encoder_states, hidden, cell"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AvDjv9os9mS"
      },
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(\n",
        "        self, input_size, embedding_size, hidden_size, output_size, p\n",
        "    ):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        self.rnn = nn.LSTM(hidden_size * 2 + embedding_size, hidden_size, 1)\n",
        "\n",
        "        self.energy = nn.Linear(hidden_size * 3, 1)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "        self.dropout = nn.Dropout(p)\n",
        "        self.softmax = nn.Softmax(dim=0)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x, encoder_states, hidden, cell):\n",
        "        x = x.unsqueeze(0)\n",
        "        # x: (1, N) where N is the batch size\n",
        "\n",
        "        embedding = self.dropout(self.embedding(x))\n",
        "        # embedding shape: (1, N, embedding_size)\n",
        "\n",
        "        sequence_length = encoder_states.shape[0]\n",
        "        h_reshaped = hidden.repeat(sequence_length, 1, 1)\n",
        "        # h_reshaped: (seq_length, N, hidden_size*2)\n",
        "\n",
        "        energy = self.relu(self.energy(torch.cat((h_reshaped, encoder_states), dim=2)))\n",
        "        # energy: (seq_length, N, 1)\n",
        "\n",
        "        attention = self.softmax(energy)\n",
        "        # attention: (seq_length, N, 1)\n",
        "\n",
        "        # attention: (seq_length, N, 1), snk\n",
        "        # encoder_states: (seq_length, N, hidden_size*2), snl\n",
        "        # we want context_vector: (1, N, hidden_size*2), i.e knl\n",
        "        context_vector = torch.einsum(\"snk,snl->knl\", attention, encoder_states)\n",
        "\n",
        "        rnn_input = torch.cat((context_vector, embedding), dim=2)\n",
        "        # rnn_input: (1, N, hidden_size*2 + embedding_size)\n",
        "\n",
        "        outputs, (hidden, cell) = self.rnn(rnn_input, (hidden, cell))\n",
        "        # outputs shape: (1, N, hidden_size)\n",
        "\n",
        "        predictions = self.fc(outputs).squeeze(0)\n",
        "        # predictions: (N, hidden_size)\n",
        "\n",
        "        return predictions, hidden, cell"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wh4XutZj4JiV"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, source, target, teacher_force_ratio=0.5):\n",
        "\n",
        "        batch_size = source.shape[1]\n",
        "        target_len = target.shape[0]\n",
        "        target_vocab_size = len(en_word2id)\n",
        "\n",
        "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
        "        encoder_states, hidden, cell = self.encoder(source)\n",
        "\n",
        "        # First input will be start token\n",
        "        x = target[0]\n",
        "\n",
        "        for t in range(1, target_len):\n",
        "            # At every time step use encoder_states and update hidden, cell\n",
        "            output, hidden, cell = self.decoder(x, encoder_states, hidden, cell)\n",
        "\n",
        "            # Store prediction for current time step\n",
        "            outputs[t] = output\n",
        "\n",
        "            # Get the best word the Decoder predicted (index in the vocabulary)\n",
        "            best_guess = output.argmax(1)\n",
        "\n",
        "            # With probability of teacher_force_ratio we take the actual next word\n",
        "            # otherwise we take the word that the Decoder predicted it to be.\n",
        "            # Teacher Forcing is used so that the model gets used to seeing\n",
        "            # similar inputs at training and testing time, if teacher forcing is 1\n",
        "            # then inputs at test time might be completely different than what the\n",
        "            # network is used to. This was a long comment.\n",
        "            x = target[t] if random.random() < teacher_force_ratio else best_guess\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def predict(self, source):\n",
        "\n",
        "        batch_size = source.shape[1]\n",
        "        source_len = source.shape[0]\n",
        "        target_vocab_size = len(en_word2id)\n",
        "\n",
        "        outputs = torch.zeros(int(1.3 * source_len), batch_size, target_vocab_size).to(device)\n",
        "        encoder_states, hidden, cell = self.encoder(source)\n",
        "\n",
        "        # First input will be <SOS> token\n",
        "        x = source[0]\n",
        "\n",
        "        for t in range(1, int(1.3 * source_len)):\n",
        "            # At every time step use encoder_states and update hidden, cell\n",
        "            output, hidden, cell = self.decoder(x, encoder_states, hidden, cell)\n",
        "\n",
        "            # Store prediction for current time step\n",
        "            outputs[t] = output\n",
        "\n",
        "            # Get the best word the Decoder predicted (index in the vocabulary)\n",
        "            best_guess = output.argmax(1)\n",
        "\n",
        "            # With probability of teacher_force_ratio we take the actual next word\n",
        "            # otherwise we take the word that the Decoder predicted it to be.\n",
        "            # Teacher Forcing is used so that the model gets used to seeing\n",
        "            # similar inputs at training and testing time, if teacher forcing is 1\n",
        "            # then inputs at test time might be completely different than what the\n",
        "            # network is used to. This was a long comment.\n",
        "            x = best_guess\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NCBpHH84i6F"
      },
      "source": [
        "load_model = False\n",
        "save_model = True\n",
        "\n",
        "# Training hyperparameters\n",
        "num_epochs = 2\n",
        "learning_rate = 1e-3\n",
        "batch_size = BATCH_SIZE\n",
        "step = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fY5Yfpj4lJX"
      },
      "source": [
        "# Model hyperparameters\n",
        "input_size_encoder = len(hi_word2id)\n",
        "input_size_decoder = len(en_word2id)\n",
        "output_size = len(en_word2id)\n",
        "encoder_embedding_size = 300\n",
        "decoder_embedding_size = 300\n",
        "hidden_size = 256\n",
        "enc_dropout = 0.5\n",
        "dec_dropout = 0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHAkXzzR4oBv"
      },
      "source": [
        "encoder_net = EncoderRNN(\n",
        "    input_size_encoder, encoder_embedding_size, hidden_size, enc_dropout\n",
        ").to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zk2xNTDs4szP"
      },
      "source": [
        "decoder_net = DecoderRNN(\n",
        "    input_size_decoder,\n",
        "    decoder_embedding_size,\n",
        "    hidden_size,\n",
        "    output_size,\n",
        "    dec_dropout,\n",
        ").to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d866kxIb4vPP"
      },
      "source": [
        "model = Seq2Seq(encoder_net, decoder_net).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptYy-TT74wE9"
      },
      "source": [
        "pad_idx = 3\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdYiUI5A44YC"
      },
      "source": [
        "model.train(True)\n",
        "\n",
        "ctr = 0\n",
        "for epoch in range(num_epochs):\n",
        "    # print(f\"[Epoch {epoch} / {num_epochs}]\")\n",
        "\n",
        "    indices = list(range(len(input_batches)))\n",
        "    random.shuffle(indices)\n",
        "\n",
        "    counter = 0\n",
        "    minibatch_X = input_batches[0]\n",
        "    minibatch_Y = output_batches[0]\n",
        "    \n",
        "    for i in indices:\n",
        "        minibatch_X = input_batches[i]\n",
        "        minibatch_Y = output_batches[i]\n",
        "        # Get input and targets and get to cuda\n",
        "        inp_data = minibatch_X\n",
        "        target = minibatch_Y\n",
        "\n",
        "        # Print batch number:\n",
        "        # print('BATCH_NUMBER:', i)\n",
        "\n",
        "        # Forward prop\n",
        "        output = model(inp_data, target, 0.5 / (1 + 0.1 * ctr))\n",
        "\n",
        "        # Output is of shape (trg_len, batch_size, output_dim) but Cross Entropy Loss\n",
        "        # doesn't take input in that form. For example if we have MNIST we want to have\n",
        "        # output to be: (N, 10) and targets just (N). Here we can view it in a similar\n",
        "        # way that we have output_words * batch_size that we want to send in into\n",
        "        # our cost function, so we need to do some reshapin. While we're at it\n",
        "        # Let's also remove the start token while we're at it\n",
        "        \n",
        "        # print('INP_SHAPE:', inp_data.shape)\n",
        "        # print('OUT_SHAPE:', target.shape)\n",
        "        # print('HYP_SHAPE:', output.shape)\n",
        "        \n",
        "        output = output[1:].reshape(-1, output.shape[2])\n",
        "        target = target[1:].reshape(-1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(output, target)\n",
        "        # print(loss)\n",
        "\n",
        "        # Back prop\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip to avoid exploding gradient issues, makes sure grads are\n",
        "        # within a healthy range\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "\n",
        "        # Gradient descent step\n",
        "        optimizer.step()\n",
        "\n",
        "        # Plot to tensorboard\n",
        "        #writer.add_scalar(\"Training loss\", loss, global_step=step)\n",
        "        ctr += 1\n",
        "        step += 1\n",
        "        counter += 1\n",
        "\n",
        "        if counter % 50 == 0:\n",
        "            print('(EPOCH, BATCH, LOSS) = ({}, {}, {})'.format(epoch, counter, loss))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IffMQc1Jo3lt"
      },
      "source": [
        "### Link to weights: [Here](https://drive.google.com/drive/folders/1BirGILg_Y68CaSp6-7jhMHfCMemUjz9U?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRsbC7IoB-04"
      },
      "source": [
        "torch.save(model.state_dict(), 'drive/MyDrive/gt_50_bi_dir_model')\n",
        "torch.save(optimizer.state_dict(), 'drive/MyDrive/gt_50_bi_dir_optimizer')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-HC7tldCge0"
      },
      "source": [
        "model.load_state_dict(torch.load('drive/MyDrive/gt_50_bi_dir_model'))\n",
        "optimizer.load_state_dict(torch.load('drive/MyDrive/gt_50_bi_dir_optimizer'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_3lL2SYdq8t"
      },
      "source": [
        "model.train(False);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7Rlt_7MU9_v"
      },
      "source": [
        "!unzip './drive/MyDrive/hindistatements.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQI0RgzwWVgN"
      },
      "source": [
        "sents = []\n",
        "with torch.no_grad():\n",
        "\n",
        "    test_data = []\n",
        "    with open('./testhindistatements.csv', 'r') as file:\n",
        "        test_data = np.array([r[2] for r in csv.reader(file)])[1::]\n",
        "\n",
        "    for sent in test_data:\n",
        "        sent = indic_tokenize.trivial_tokenize(sent)\n",
        "        query = get_indices_seq(sent, hi_word2id).reshape(-1, 1)\n",
        "        response = model.predict(query.to(device))\n",
        "\n",
        "        # print('===================================================================')\n",
        "        # print(' '.join(sent))\n",
        "        r = get_word_seq(torch.argmax(response[:, 0, :], dim=1), en_id2word)\n",
        "        if r[-1] == '__<<stop>>__':\n",
        "            sents += [' '.join(r[1:r.index('__<<stop>>__')]) + '\\n']\n",
        "        else:\n",
        "            sents += [' '.join(r[1::]) + '\\n']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0VAdlFLeFsF"
      },
      "source": [
        "in_data = []\n",
        "with open('./testhindistatements.csv', 'r') as file:\n",
        "    in_data = np.array([r[2] for r in csv.reader(file)])[1::]\n",
        "\n",
        "for i in range(len(in_data)):\n",
        "    print(in_data[i])\n",
        "    print(sents[i])\n",
        "    print('===================================================================')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ee5WEkoLUwJ-"
      },
      "source": [
        "with open('answer.txt', 'w') as file:\n",
        "    file.writelines(sents)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXQToWjKU9yl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e811f91f-9f90-4393-f9ef-834b6b0e3dfb"
      },
      "source": [
        "!zip GT_20111407.zip answer.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: answer.txt (deflated 69%)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}